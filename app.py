import os
from dotenv import load_dotenv
from flask import Flask, request, abort
from linebot.v3.webhook import WebhookHandler, Event
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging. models import TextMessage
from linebot import LineBotApi, WebhookHandler
from linebot.models import (
    MessageEvent, 
    TextMessage, 
    TextSendMessage,
    ImageSendMessage)
from linebot.exceptions import InvalidSignatureError
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# åŠ è¼‰ .env æ–‡ä»¶ä¸­çš„è®Šæ•¸
load_dotenv()

# å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– LINE çš„ Channel Access Token å’Œ Channel Secret
line_token = os.getenv('LINE_TOKEN')
line_secret = os.getenv('LINE_SECRET')

# æª¢æŸ¥æ˜¯å¦è¨­ç½®äº†ç’°å¢ƒè®Šæ•¸
if not line_token or not line_secret:  
    print(f"LINE_TOKEN:   {line_token}")
    print(f"LINE_SECRET:  {line_secret}")
    raise ValueError("LINE_TOKEN æˆ– LINE_SECRET æœªè¨­ç½®")

# åˆå§‹åŒ– LineBotApi å’Œ WebhookHandler
line_bot_api = LineBotApi(line_token)
handler = WebhookHandler(line_secret)

# å‰µå»º Flask æ‡‰ç”¨
app = Flask(__name__)
app.logger.setLevel(logging.DEBUG)

# ===== ç¶²ç«™è³‡è¨Šé…ç½® =====
WEBSITE_URL = "https://pumentea.vercel.app/"

# å…·é«”é é¢ URLsï¼ˆè«‹æ ¹æ“šä½ çš„ç¶²ç«™çµæ§‹èª¿æ•´ï¼‰
PAGE_URLS = {
    'home': 'https://pumentea.vercel.app/',
    'menu': 'https://pumentea.vercel.app/prodcuts',  # menu å’Œ products ä½¿ç”¨ç›¸åŒåœ°å€
    'products': 'https://pumentea.vercel.app/prodcuts',  # menu å’Œ products ä½¿ç”¨ç›¸åŒåœ°å€
    'about': 'https://pumentea.vercel.app/about',  # å¦‚æœæ˜¯ç¨ç«‹é é¢æ”¹ç‚º /about
    'contact': 'https://pumentea.vercel.app/store',  # å¦‚æœæ˜¯ç¨ç«‹é é¢æ”¹ç‚º /contact
}

# LINE è‡ªå‹•å›è¦†è™•ç†çš„é—œéµå­—ï¼ˆé€™äº›è¨Šæ¯ä¸ç”± bot å›æ‡‰ï¼‰
AUTO_REPLY_KEYWORDS = [
    # ç‡Ÿæ¥­æ™‚é–“ç›¸é—œ
    'ç‡Ÿæ¥­æ™‚é–“', 'opening time', 'opening hours', 'å¹¾é»', 'é–‹åº—', 'é—œé–€',
    'ä»Šå¤©æœ‰é–‹', 'æœ‰é–‹å—', 'é–‹é–€', 'ç‡Ÿæ¥­', 'ä¼‘æ¯', 'å…¬ä¼‘', 'é–‹åˆ°å¹¾é»',
    'ä»€éº¼æ™‚å€™é–‹', 'å¹¾é»é–‹', 'å¹¾é»é—œ',
    
    # åœ°å€ç›¸é—œ
    'åœ°å€', 'address', 'location', 'åœ¨å“ªè£¡', 'åœ¨å“ª', 'where',
    'ä½ç½®', 'æ€éº¼å»', 'å¦‚ä½•åˆ°é”', 'åº—åœ¨å“ª', 'æ€éº¼èµ°'
]

# ç°¡å–®çš„å¿«å–æ©Ÿåˆ¶ï¼ˆé¿å…æ¯æ¬¡è«‹æ±‚éƒ½æŠ“å–ç¶²ç«™ï¼‰
website_cache = {
    'data':  None,
    'timestamp':  0
}
CACHE_DURATION = 3600  # 1å°æ™‚å¿«å–

def fetch_website_info():
    """æŠ“å–ç¶²ç«™å…§å®¹"""
    import time
    current_time = time.time()
    
    # æª¢æŸ¥å¿«å–
    if website_cache['data'] and (current_time - website_cache['timestamp']) < CACHE_DURATION:
        app.logger.info("ä½¿ç”¨å¿«å–çš„ç¶²ç«™è³‡è¨Š")
        return website_cache['data']
    
    try:
        app.logger.info(f"æ­£åœ¨æŠ“å–ç¶²ç«™å…§å®¹:   {WEBSITE_URL}")
        response = requests.get(WEBSITE_URL, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–ç¶²ç«™åŸºæœ¬è³‡è¨Š
        info = {
            'title': '',
            'description': '',
            'products': [],
            'links': [],
            'text_content': ''
        }
        
        # ç²å–æ¨™é¡Œ
        title_tag = soup.find('title')
        if title_tag:  
            info['title'] = title_tag.text.strip()
        
        # ç²å–æè¿°
        description_tag = soup.find('meta', {'name': 'description'})
        if description_tag and description_tag.get('content'):
            info['description'] = description_tag['content']
        
        # ç²å–æ‰€æœ‰æ–‡å­—å…§å®¹ï¼ˆç”¨æ–¼é—œéµå­—æœå°‹ï¼‰
        body = soup.find('body')
        if body:
            # ç§»é™¤ script å’Œ style æ¨™ç±¤
            for script in body(['script', 'style']):
                script.decompose()
            info['text_content'] = body.get_text(separator=' ', strip=True)
        
        # å˜—è©¦æ‰¾ç”¢å“ç›¸é—œè³‡è¨Š
        # æ–¹æ³•1: å°‹æ‰¾åŒ…å«åƒ¹æ ¼ç¬¦è™Ÿçš„å…ƒç´ 
        price_elements = soup.find_all(string=lambda text: text and ('NT$' in text or '$' in text or 'å…ƒ' in text))
        for elem in price_elements[: 10]:  # æœ€å¤šå–10å€‹
            parent = elem.parent
            if parent:  
                product_text = parent.get_text(strip=True)
                if len(product_text) < 200:  # é¿å…å–åˆ°å¤ªé•·çš„æ–‡å­—
                    info['products'].append(product_text)
        
        # æ–¹æ³•2: å°‹æ‰¾æ¨™é¡Œæ¨™ç±¤ï¼ˆå¯èƒ½æ˜¯ç”¢å“åç¨±ï¼‰
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4'])
        for heading in headings[: 10]:  
            heading_text = heading.get_text(strip=True)
            if heading_text and len(heading_text) < 100:
                info['products'].append(heading_text)
        
        # ç²å–é‡è¦é€£çµ
        links = soup. find_all('a', href=True)
        for link in links[:5]:  
            link_text = link.get_text(strip=True)
            link_url = urljoin(WEBSITE_URL, link['href'])
            if link_text:  
                info['links'].append({'text': link_text, 'url': link_url})
        
        # æ›´æ–°å¿«å–
        website_cache['data'] = info
        website_cache['timestamp'] = current_time
        
        app.logger.info(f"æˆåŠŸæŠ“å–ç¶²ç«™è³‡è¨Š: {len(info['products'])} å€‹ç”¢å“é …ç›®")
        return info
        
    except requests.RequestException as e:
        app.logger.error(f"æŠ“å–ç¶²ç«™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None
    except Exception as e:
        app.logger. error(f"è™•ç†ç¶²ç«™å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def search_in_website(keyword, website_info):
    """åœ¨ç¶²ç«™å…§å®¹ä¸­æœå°‹é—œéµå­—"""
    if not website_info:  
        return None
    
    keyword_lower = keyword.lower()
    results = []
    
    # åœ¨ç”¢å“ä¸­æœå°‹
    for product in website_info['products']:
        if keyword_lower in product.lower():
            results.append(product)
    
    # åœ¨å…¨æ–‡ä¸­æœå°‹
    if keyword_lower in website_info['text_content'].lower():
        # æ‰¾åˆ°é—œéµå­—é™„è¿‘çš„æ–‡å­—
        text = website_info['text_content']
        index = text.lower().find(keyword_lower)
        if index != -1:
            start = max(0, index - 50)
            end = min(len(text), index + 100)
            context = text[start:end].strip()
            results.append(f"...  {context}...")
    
    return results[: 5]  # æœ€å¤šè¿”å›5å€‹çµæœ

def generate_response(user_message):
    """æ ¹æ“šä½¿ç”¨è€…è¨Šæ¯å’Œç¶²ç«™è³‡è¨Šç”Ÿæˆå›æ‡‰"""
    message_lower = user_message.lower()
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºè‡ªå‹•å›è¦†é—œéµå­—ï¼ˆè¿”å› None è®“ LINE è‡ªå‹•å›è¦†è™•ç†ï¼‰
    if any(keyword in message_lower for keyword in AUTO_REPLY_KEYWORDS):
        app.logger.info(f"åµæ¸¬åˆ°è‡ªå‹•å›è¦†é—œéµå­—ï¼Œä¸å›æ‡‰:  {user_message}")
        return None
    
    # å•å€™èª
    if any(keyword in message_lower for keyword in ['hi', 'hello', 'ä½ å¥½', 'å—¨', 'å“ˆå›‰', 'hey', 'å˜¿']):
        return f"ğŸ‘‹ æ‚¨å¥½ï¼æ­¡è¿ä¾†åˆ°æ™®é–€èŒ¶å“ï¼\n\næˆ‘å¯ä»¥å¹«æ‚¨ï¼š\nâ€¢ æŸ¥çœ‹èœå–®ï¼ˆè¼¸å…¥ã€Œèœå–®ã€ï¼‰\nâ€¢ æœå°‹ç”¢å“ï¼ˆè¼¸å…¥ç”¢å“åç¨±ï¼‰\nâ€¢ äº†è§£é—œæ–¼æˆ‘å€‘ï¼ˆè¼¸å…¥ã€Œé—œæ–¼ã€ï¼‰\nâ€¢ æŸ¥è©¢ç‡Ÿæ¥­æ™‚é–“ï¼ˆè¼¸å…¥ã€Œç‡Ÿæ¥­æ™‚é–“ã€ï¼‰\nâ€¢ æŸ¥è©¢åœ°å€ï¼ˆè¼¸å…¥ã€Œåœ°å€ã€ï¼‰\n\nğŸŒ å®˜ç¶²é¦–é ï¼š\n{PAGE_URLS['home']}"
    
    # èœå–®/ç”¢å“æŸ¥è©¢ - ä½¿ç”¨ç›¸åŒçš„ menu åœ°å€
    elif any(keyword in message_lower for keyword in ['menu', 'èœå–®', 'product', 'ç”¢å“', 'tea', 'èŒ¶', 'å•†å“', 'æ™®é–€']):
        website_info = fetch_website_info()
        if website_info and website_info['products']:
            products_text = "\nâ€¢ ".join(website_info['products'][:8])  # é¡¯ç¤ºå‰8å€‹é …ç›®
            return f"ğŸµ {website_info['title']}\n\næˆ‘å€‘çš„ç”¢å“ï¼š\nâ€¢ {products_text}\n\nğŸ“‹ å®Œæ•´èœå–®è«‹è¨ªå•ï¼š\n{PAGE_URLS['menu']}"
        else:
            return f"ğŸµ ç’é–€èŒ¶èœå–®\n\næŸ¥çœ‹å®Œæ•´èœå–®ï¼š\n{PAGE_URLS['menu']}"
    
    # é—œæ–¼æˆ‘å€‘ - è¿”å›å…·é«” about é é¢
    elif any(keyword in message_lower for keyword in ['about', 'é—œæ–¼', 'ä»‹ç´¹', 'ç°¡ä»‹', 'about us']):
        website_info = fetch_website_info()
        response = f"ğŸ“– é—œæ–¼æ™®é–€èŒ¶å“\n\n"
        if website_info:  
            if website_info['description']:
                response += f"{website_info['description']}\n\n"
        response += f"ğŸ”— äº†è§£æ›´å¤šé—œæ–¼æˆ‘å€‘ï¼š\n{PAGE_URLS['about']}"
        return response
    
    # åƒ¹æ ¼æŸ¥è©¢
    elif any(keyword in message_lower for keyword in ['price', 'åƒ¹æ ¼', 'å¤šå°‘éŒ¢', 'how much', 'è²»ç”¨']):
        website_info = fetch_website_info()
        if website_info: 
            price_items = [p for p in website_info['products'] if any(symbol in p for symbol in ['$', 'NT', 'å…ƒ'])]
            if price_items:
                price_text = "\nâ€¢ ".join(price_items[: 5])
                return f"ğŸ’° åƒ¹æ ¼è³‡è¨Šï¼š\n\nâ€¢ {price_text}\n\nå®Œæ•´åƒ¹æ ¼è«‹è¨ªå•ï¼š\n{PAGE_URLS['menu']}"
        return f"ğŸ’° åƒ¹æ ¼è³‡è¨Šè«‹è¨ªå•æˆ‘å€‘çš„èœå–®é é¢ï¼š\n{PAGE_URLS['menu']}"
    
    # è¯çµ¡æ–¹å¼
    elif any(keyword in message_lower for keyword in ['contact', 'è¯çµ¡', 'è¯ç¹«', 'call', 'é›»è©±']):
        return f"ğŸ“ è¯çµ¡æˆ‘å€‘ï¼š\n\nè«‹è¨ªå•è¯çµ¡é é¢äº†è§£æ›´å¤šï¼š\n{PAGE_URLS['contact']}\n\næˆ–ç›´æ¥åœ¨ LINE ä¸Šç•™è¨€ï¼Œæˆ‘å€‘æœƒç›¡å¿«å›è¦†ï¼"
    
    # è¨‚è³¼
    elif any(keyword in message_lower for keyword in ['order', 'è¨‚è³¼', 'buy', 'è³¼è²·', 'è¨‚å–®']):
        return f"ğŸ›’ è¨‚è³¼æ–¹å¼ï¼š\n\n1. ç·šä¸Šè¨‚è³¼ï¼š{PAGE_URLS['menu']}\n2. åœ¨ LINE å‘Šè¨´æˆ‘å€‘æ‚¨æƒ³è¦çš„å•†å“\n3. è¯ç¹«å®¢æœ\n\néœ€è¦ä»€éº¼å”åŠ©å—ï¼Ÿ"
    
    # æœå°‹åŠŸèƒ½
    elif any(keyword in message_lower for keyword in ['æœå°‹', 'search', 'æ‰¾', 'æŸ¥']):
        return f"ğŸ” è«‹å‘Šè¨´æˆ‘æ‚¨æƒ³æœå°‹ä»€éº¼ï¼Ÿ\n\nä¾‹å¦‚ï¼šã€Œçƒé¾èŒ¶ã€ã€ã€Œç´…èŒ¶ã€ã€ã€Œåƒ¹æ ¼ã€ç­‰\n\næˆ–ç›´æ¥è¨ªå•å®˜ç¶²ï¼š\n{WEBSITE_URL}"
    
    # ä¸€èˆ¬æœå°‹ï¼ˆç•¶è¨Šæ¯ä¸æ˜¯ç‰¹å®šå‘½ä»¤æ™‚ï¼‰
    else:  
        website_info = fetch_website_info()
        if website_info: 
            # å˜—è©¦åœ¨ç¶²ç«™å…§å®¹ä¸­æœå°‹ä½¿ç”¨è€…çš„è¨Šæ¯
            search_results = search_in_website(user_message, website_info)
            if search_results:
                results_text = "\n\nâ€¢ ".join(search_results)
                return f"ğŸ” æ‰¾åˆ°ç›¸é—œè³‡è¨Šï¼š\n\nâ€¢ {results_text}\n\næ›´å¤šè©³æƒ…ï¼š\n{PAGE_URLS['menu']}"
        
        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œçµ¦äºˆå‹å–„å›æ‡‰
        return f"è¬è¬æ‚¨çš„è¨Šæ¯ï¼šã€Œ{user_message}ã€\n\nå¦‚éœ€äº†è§£æ›´å¤šï¼Œè«‹ï¼š\nâ€¢ è¼¸å…¥ã€Œèœå–®ã€æŸ¥çœ‹ç”¢å“\nâ€¢ è¼¸å…¥ã€Œé—œæ–¼ã€äº†è§£æˆ‘å€‘\nâ€¢ è¼¸å…¥ã€Œç‡Ÿæ¥­æ™‚é–“ã€æŸ¥è©¢é–‹åº—æ™‚é–“\nâ€¢ è¼¸å…¥ã€Œåœ°å€ã€æŸ¥è©¢åº—é¢ä½ç½®\nâ€¢ è¨ªå•å®˜ç¶²ï¼š{WEBSITE_URL}\n\né‚„æœ‰ä»€éº¼æˆ‘å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿ"

# è¨­ç½®ä¸€å€‹è·¯ç”±ä¾†è™•ç† LINE Webhook çš„å›èª¿è«‹æ±‚
@app.route("/", methods=['POST'])
def callback():
    # å–å¾— X-Line-Signature æ¨™é ­
    signature = request. headers['X-Line-Signature']

    # å–å¾—è«‹æ±‚çš„åŸå§‹å…§å®¹
    body = request.get_data(as_text=True)
    app.logger.info(f"Request body: {body}")

    # é©—è­‰ç°½åä¸¦è™•ç†è«‹æ±‚
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:  
        abort(400)

    return 'OK'

# å¥åº·æª¢æŸ¥è·¯ç”±ï¼ˆRender éœ€è¦ï¼‰
@app.route("/", methods=['GET'])
def health_check():
    return 'LINE Bot is running! ', 200

# è¨­ç½®ä¸€å€‹äº‹ä»¶è™•ç†å™¨ä¾†è™•ç† TextMessage äº‹ä»¶
@handler.add(MessageEvent, message=TextMessage)
def handle_message(event:  Event):
    if event.message.type == "text":
        user_message = event.message.text  # ä½¿ç”¨è€…çš„è¨Šæ¯
        app.logger.info(f"æ”¶åˆ°çš„è¨Šæ¯: {user_message}")

        # ä½¿ç”¨ç¶²ç«™è³‡è¨Šç”Ÿæˆå›æ‡‰
        reply_text = generate_response(user_message)
        
        # åªæœ‰åœ¨æœ‰å›æ‡‰æ™‚æ‰ç™¼é€è¨Šæ¯ï¼ˆNone è¡¨ç¤ºç”± LINE è‡ªå‹•å›è¦†è™•ç†ï¼‰
        if reply_text:
            line_bot_api.reply_message(
                event.reply_token,
                TextSendMessage(text=reply_text)
            )
        else:
            app.logger.info("è®“ LINE è‡ªå‹•å›è¦†è™•ç†æ­¤è¨Šæ¯")

# æ‡‰ç”¨ç¨‹åºå…¥å£é»
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=5000)




